{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q -U albumentations\n!pip install -q -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:29:06.670049Z","iopub.execute_input":"2024-03-17T14:29:06.670404Z","iopub.status.idle":"2024-03-17T14:29:53.994722Z","shell.execute_reply.started":"2024-03-17T14:29:06.670374Z","shell.execute_reply":"2024-03-17T14:29:53.993464Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.1 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.1 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch \nfrom pathlib import Path\nfrom PIL import Image\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision.datasets import MNIST\n\nimport os\nimport csv\nfrom torchvision import transforms\nimport albumentations\nfrom typing import Tuple, List, Optional","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:29:53.997047Z","iopub.execute_input":"2024-03-17T14:29:53.997599Z","iopub.status.idle":"2024-03-17T14:30:04.647268Z","shell.execute_reply.started":"2024-03-17T14:29:53.997570Z","shell.execute_reply":"2024-03-17T14:30:04.646182Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/VAaRrUNn/HumanAI_Gsoc.git","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:30:04.648584Z","iopub.execute_input":"2024-03-17T14:30:04.649053Z","iopub.status.idle":"2024-03-17T14:30:07.106581Z","shell.execute_reply.started":"2024-03-17T14:30:04.649025Z","shell.execute_reply":"2024-03-17T14:30:07.105412Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'HumanAI_Gsoc'...\nremote: Enumerating objects: 35, done.\u001b[K\nremote: Counting objects: 100% (35/35), done.\u001b[K\nremote: Compressing objects: 100% (31/31), done.\u001b[K\nremote: Total 35 (delta 3), reused 29 (delta 1), pack-reused 0\u001b[K\nUnpacking objects: 100% (35/35), 7.77 KiB | 995.00 KiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd HumanAI_Gsoc/Data","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:30:07.109197Z","iopub.execute_input":"2024-03-17T14:30:07.109514Z","iopub.status.idle":"2024-03-17T14:30:07.116775Z","shell.execute_reply.started":"2024-03-17T14:30:07.109486Z","shell.execute_reply":"2024-03-17T14:30:07.115831Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/HumanAI_Gsoc/Data\n","output_type":"stream"}]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:30:07.117967Z","iopub.execute_input":"2024-03-17T14:30:07.118258Z","iopub.status.idle":"2024-03-17T14:30:08.138293Z","shell.execute_reply.started":"2024-03-17T14:30:07.118234Z","shell.execute_reply":"2024-03-17T14:30:08.136947Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"dataset.csv  \u001b[0m\u001b[01;34mimages\u001b[0m/  text.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"class temp(Dataset):\n    def __init__(self):\n        super().__init__()\n        \n    def __len__(self):\n        return 10;\n        \n    def __getitem__(self, idx):\n        return 10, 20\n    \ntd = temp()\n\ndef wow(x):\n    print(\"In wow\")\n    print(x)\n    print(\"Out of wow\")\ntdl = DataLoader(dataset = td,\n                batch_size = 2,\n                collate_fn = wow)\n\nnext(iter(tdl))","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:30:08.140036Z","iopub.execute_input":"2024-03-17T14:30:08.140397Z","iopub.status.idle":"2024-03-17T14:30:08.165156Z","shell.execute_reply.started":"2024-03-17T14:30:08.140367Z","shell.execute_reply":"2024-03-17T14:30:08.164233Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"In wow\n[(10, 20), (10, 20)]\nOut of wow\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\ntokenizer = AutoTokenizer.from_pretrained(\"DeepESP/gpt2-spanish\")\ndata_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n\nsen = [\"what are you doing\", \"My name is varun \"]\nt1 = tokenizer(\"what are you doing\")\nt2 = tokenizer(\"My name is varun \")\ntt = tokenizer(sen)\n# data_collator([t1, t2])\ndata_collator(tt)","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:33:30.069851Z","iopub.execute_input":"2024-03-17T14:33:30.070518Z","iopub.status.idle":"2024-03-17T14:33:30.293393Z","shell.execute_reply.started":"2024-03-17T14:33:30.070453Z","shell.execute_reply":"2024-03-17T14:33:30.292220Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[   96,  6076, 24554, 17257,   728,   640, 50256],\n        [12330,   309,  2002,  1672,   958,   290,   230]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0],\n        [1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Making dataset\nDataset structure:\n#### 1. images/\n- this contains all the images in jpeg format\n#### 2. dataset.csv\n- It contains 2 cols\n- This contains the images name which are present in the images/ folder and their corrosponding text","metadata":{}},{"cell_type":"code","source":"class OCR_temp(Dataset):\n    def __init__(self,\n                images,\n                labels,\n                transforms = None,\n                tokenizer = None):\n        super().__init__()\n        self.images = images\n        self.labels = labels\n        \n        self.transforms = transforms\n        self.tokenizer = tokenizer\n        \n    def __len__(self,) -> int:\n        return len(self.images)\n    \n    def __getitem__(self,\n                   idx) -> Tuple:\n        img, lab = Image.open(self.images[idx]), self.labels[idx]\n        if self.transforms:\n            img = self.transforms(img)\n        \n        # The padding will be done in coll_fn in dataloader\n        lab = self.tokenizer(lab)\n        \n        return (img, lab)\n        \n        \n\nclass OCRDataModule(pl.LightningDataModule):\n    \n    def __init__(self,\n                data_dir: str,\n                csv_file,\n                transforms,\n                batch_size = 5,\n                tokenizer = None,\n                data_collator = None):\n        super().__init__()\n        self.batch_size = batch_size\n        self.data_dir = Path(data_dir)\n        self.csv_file = csv_file \n        \n        self.transforms = transforms\n        self.tokenizer = tokenizer\n        self.data_collator = data_collator\n        \n        \n    def prepare_data(self):\n        \"\"\"\n        Some Preprocessing\n        \"\"\"\n        images_name, self.labels = [], []\n        with open(\"dataset.csv\", 'r') as f:\n            content = csv.reader(f)\n            \n            # Skipping 1st row\n            next(content)\n            \n            for row in content:\n                images_name.append(row[0])\n                self.labels.append(row[1])\n        \n        \n        \n        # Getting absolute path for images\n        self.images = [\n            self.data_dir / Path(\"images\") / x for x in images_name\n        ]\n        \n    \n    def setup(self,\n             stage: str = \"train\"):\n        if stage == \"fit\" or \"validate\":\n            self.data = OCR_temp(images = self.images,\n                            labels = self.labels,\n                            transforms = self.transforms,\n                            tokenizer = self.tokenizer)\n        \n        if stage == \"test\":\n            \"\"\"\n            Currently not implemented, because the (current) dataset size is very small\n            \"\"\"\n            pass\n    \n    def __collate_fn(self,\n                    samples) -> Tuple:\n        images, labels = [], []\n        for (img, lab) in samples:\n            images.append(img)\n            labels.append(lab)\n        labels = self.data_collator(labels)\n        return (images, labels)\n            \n        \n            \n    def train_dataloader(self):\n        return DataLoader(dataset = self.data,\n                         batch_size = self.batch_size,\n                         shuffle = True,\n                         collate_fn = self.__collate_fn)\n        \n    def val_dataloader(self):\n        pass\n    \n    def test_dataloader(self):\n        pass\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:40:00.396887Z","iopub.execute_input":"2024-03-17T14:40:00.397719Z","iopub.status.idle":"2024-03-17T14:40:00.414920Z","shell.execute_reply.started":"2024-03-17T14:40:00.397685Z","shell.execute_reply":"2024-03-17T14:40:00.413865Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\ntokenizer = AutoTokenizer.from_pretrained(\"DeepESP/gpt2-spanish\")\ndata_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n\n# some testing\n# t1 = tokenizer(\"what are you doing\")\n# t2 = tokenizer(\"My name is varun \")\n# data_collator([t1, t2])\n\ntrain_transform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\ntdataset = OCRDataModule(data_dir = \".\", # currently it's . because I am already in the data dir, but will not be the case in general\n             csv_file = \"dataset.csv\",\n             transforms = train_transform,\n             tokenizer = tokenizer,\n             data_collator = data_collator)\n\n\ntdataset.prepare_data()\ntdataset.setup()\n\nnext(iter(tdataset.train_dataloader()))","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:40:00.849476Z","iopub.execute_input":"2024-03-17T14:40:00.849857Z","iopub.status.idle":"2024-03-17T14:40:01.656919Z","shell.execute_reply.started":"2024-03-17T14:40:00.849829Z","shell.execute_reply":"2024-03-17T14:40:01.655861Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"([tensor([[[0.0157, 0.0157, 0.0157,  ..., 0.0118, 0.0314, 0.0314],\n           [0.0118, 0.0118, 0.0118,  ..., 0.0118, 0.0314, 0.0314],\n           [0.0471, 0.0471, 0.0471,  ..., 0.0118, 0.0353, 0.0353],\n           ...,\n           [0.0314, 0.0235, 0.0118,  ..., 0.0196, 0.0314, 0.0353],\n           [0.0353, 0.0353, 0.0235,  ..., 0.0039, 0.0157, 0.0157],\n           [0.0353, 0.0353, 0.0235,  ..., 0.0039, 0.0157, 0.0157]],\n  \n          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0275, 0.0275, 0.0275,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0157, 0.0078, 0.0000,  ..., 0.0157, 0.0157, 0.0157],\n           [0.0157, 0.0157, 0.0078,  ..., 0.0000, 0.0000, 0.0039],\n           [0.0157, 0.0157, 0.0078,  ..., 0.0000, 0.0000, 0.0039]],\n  \n          [[0.0118, 0.0118, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0078, 0.0078, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0510, 0.0510, 0.0510,  ..., 0.0000, 0.0039, 0.0039],\n           ...,\n           [0.0824, 0.0745, 0.0627,  ..., 0.0078, 0.0118, 0.0039],\n           [0.0941, 0.0941, 0.0745,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0941, 0.0941, 0.0745,  ..., 0.0000, 0.0000, 0.0000]]]),\n  tensor([[[0.0039, 0.0039, 0.0039,  ..., 0.0157, 0.0157, 0.0118],\n           [0.0039, 0.0039, 0.0039,  ..., 0.0118, 0.0118, 0.0078],\n           [0.0039, 0.0039, 0.0039,  ..., 0.0078, 0.0078, 0.0039],\n           ...,\n           [0.0118, 0.0078, 0.0078,  ..., 0.0078, 0.0039, 0.0039],\n           [0.0118, 0.0118, 0.0078,  ..., 0.0078, 0.0039, 0.0039],\n           [0.0118, 0.0118, 0.0118,  ..., 0.0078, 0.0039, 0.0039]],\n  \n          [[0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0196, 0.0157],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0157, 0.0118],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0118, 0.0078],\n           ...,\n           [0.0118, 0.0078, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0118, 0.0118, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0118, 0.0118, 0.0118,  ..., 0.0000, 0.0000, 0.0000]],\n  \n          [[0.0000, 0.0000, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0118, 0.0078, 0.0078,  ..., 0.0039, 0.0000, 0.0000],\n           [0.0118, 0.0118, 0.0078,  ..., 0.0039, 0.0000, 0.0000],\n           [0.0118, 0.0118, 0.0118,  ..., 0.0039, 0.0000, 0.0000]]]),\n  tensor([[[0.1961, 0.1961, 0.1961,  ..., 0.2353, 0.2275, 0.2235],\n           [0.1961, 0.1961, 0.1961,  ..., 0.2353, 0.2275, 0.2235],\n           [0.1961, 0.1961, 0.1961,  ..., 0.2314, 0.2275, 0.2235],\n           ...,\n           [0.1961, 0.1961, 0.1961,  ..., 0.2824, 0.2824, 0.2824],\n           [0.1961, 0.1961, 0.1961,  ..., 0.2000, 0.2000, 0.2000],\n           [0.1961, 0.1961, 0.1961,  ..., 0.2039, 0.2000, 0.2000]],\n  \n          [[0.1490, 0.1490, 0.1490,  ..., 0.2078, 0.2000, 0.2039],\n           [0.1490, 0.1490, 0.1490,  ..., 0.2078, 0.2000, 0.2039],\n           [0.1490, 0.1490, 0.1490,  ..., 0.2039, 0.2000, 0.2039],\n           ...,\n           [0.1725, 0.1725, 0.1725,  ..., 0.2471, 0.2588, 0.2588],\n           [0.1686, 0.1686, 0.1725,  ..., 0.1647, 0.1765, 0.1765],\n           [0.1686, 0.1686, 0.1725,  ..., 0.1686, 0.1725, 0.1765]],\n  \n          [[0.2275, 0.2275, 0.2275,  ..., 0.2392, 0.2314, 0.2314],\n           [0.2275, 0.2275, 0.2275,  ..., 0.2392, 0.2314, 0.2314],\n           [0.2275, 0.2275, 0.2275,  ..., 0.2353, 0.2314, 0.2314],\n           ...,\n           [0.2824, 0.2824, 0.2824,  ..., 0.3137, 0.3137, 0.3137],\n           [0.2902, 0.2902, 0.2824,  ..., 0.2314, 0.2314, 0.2314],\n           [0.2902, 0.2902, 0.2824,  ..., 0.2353, 0.2353, 0.2314]]]),\n  tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0078, 0.0078, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0078, 0.0078, 0.0078,  ..., 0.0000, 0.0000, 0.0000]],\n  \n          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0078, 0.0078, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0078, 0.0078, 0.0078,  ..., 0.0000, 0.0000, 0.0000]],\n  \n          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0078, 0.0078, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0078, 0.0078, 0.0078,  ..., 0.0000, 0.0000, 0.0000]]]),\n  tensor([[[0.0196, 0.0157, 0.0118,  ..., 0.0039, 0.0000, 0.0000],\n           [0.0196, 0.0157, 0.0118,  ..., 0.0039, 0.0000, 0.0000],\n           [0.0196, 0.0157, 0.0118,  ..., 0.0039, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n  \n          [[0.0157, 0.0118, 0.0078,  ..., 0.0039, 0.0000, 0.0000],\n           [0.0157, 0.0118, 0.0078,  ..., 0.0039, 0.0000, 0.0000],\n           [0.0157, 0.0118, 0.0078,  ..., 0.0039, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],\n  \n          [[0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0000],\n           ...,\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])],\n {'input_ids': tensor([[  324,   831,   286,  ..., 50256, 50256, 50256],\n         [  338,   545,   746,  ..., 50256, 50256, 50256],\n         [  395,  2309,   366,  ...,   286,    21,   230],\n         [ 2012,  6297,   352,  ..., 50256, 50256, 50256],\n         [  572,  6906,   396,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 1, 1, 1],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0]])})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}